{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attitude detection for inappropriate\n",
    "- model: kcbert-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parameter 'function'=<function tokenize_function at 0x7f2924b05a80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 13/13 [00:01<00:00, 12.12ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 16.16ba/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m-zero\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/link/nlp/Attitude-detection-for-inappropriate-sentences/wandb/run-20240615_173450-ywnbq7kn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/-zero/huggingface/runs/ywnbq7kn' target=\"_blank\">test_trainer</a></strong> to <a href='https://wandb.ai/-zero/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/-zero/huggingface' target=\"_blank\">https://wandb.ai/-zero/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/-zero/huggingface/runs/ywnbq7kn' target=\"_blank\">https://wandb.ai/-zero/huggingface/runs/ywnbq7kn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16240' max='16240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16240/16240 1:08:40, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.312591</td>\n",
       "      <td>0.883005</td>\n",
       "      <td>0.882781</td>\n",
       "      <td>0.883005</td>\n",
       "      <td>0.832135</td>\n",
       "      <td>0.883005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.322100</td>\n",
       "      <td>0.297201</td>\n",
       "      <td>0.895936</td>\n",
       "      <td>0.892557</td>\n",
       "      <td>0.895936</td>\n",
       "      <td>0.840927</td>\n",
       "      <td>0.895936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.292400</td>\n",
       "      <td>0.273224</td>\n",
       "      <td>0.897783</td>\n",
       "      <td>0.894766</td>\n",
       "      <td>0.897783</td>\n",
       "      <td>0.841454</td>\n",
       "      <td>0.897783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.302600</td>\n",
       "      <td>0.289258</td>\n",
       "      <td>0.897167</td>\n",
       "      <td>0.893932</td>\n",
       "      <td>0.897167</td>\n",
       "      <td>0.844184</td>\n",
       "      <td>0.897167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.288800</td>\n",
       "      <td>0.276797</td>\n",
       "      <td>0.900246</td>\n",
       "      <td>0.897285</td>\n",
       "      <td>0.900246</td>\n",
       "      <td>0.849665</td>\n",
       "      <td>0.900246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.294756</td>\n",
       "      <td>0.899015</td>\n",
       "      <td>0.897203</td>\n",
       "      <td>0.899015</td>\n",
       "      <td>0.852198</td>\n",
       "      <td>0.899015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.269900</td>\n",
       "      <td>0.292666</td>\n",
       "      <td>0.902094</td>\n",
       "      <td>0.899260</td>\n",
       "      <td>0.902094</td>\n",
       "      <td>0.852608</td>\n",
       "      <td>0.902094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.299843</td>\n",
       "      <td>0.901478</td>\n",
       "      <td>0.898720</td>\n",
       "      <td>0.901478</td>\n",
       "      <td>0.852471</td>\n",
       "      <td>0.901478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.298238</td>\n",
       "      <td>0.899631</td>\n",
       "      <td>0.896615</td>\n",
       "      <td>0.899631</td>\n",
       "      <td>0.848574</td>\n",
       "      <td>0.899631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.299038</td>\n",
       "      <td>0.901478</td>\n",
       "      <td>0.899077</td>\n",
       "      <td>0.901478</td>\n",
       "      <td>0.854014</td>\n",
       "      <td>0.901478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 16.14ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"nikluge-2023-iau-test-000001\", \"input\": \"아니 진짜 미친놈아니에요?????\", \"output\": \"NEGATIVE\"}\n",
      "{\"id\": \"nikluge-2023-iau-test-000002\", \"input\": \"아진심 미쳘냐공ㄱ\", \"output\": \"POSITIVE\"}\n",
      "{\"id\": \"nikluge-2023-iau-test-000003\", \"input\": \"먹고후회할바엔 먹지말자 ㅅㅂ\", \"output\": \"NEGATIVE\"}\n",
      "{\"id\": \"nikluge-2023-iau-test-000004\", \"input\": \"심멎사진 나갑니다\", \"output\": \"POSITIVE\"}\n",
      "{\"id\": \"nikluge-2023-iau-test-000005\", \"input\": \"아시발너무 ..\", \"output\": \"NEGATIVE\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model\n",
    "\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        data = [json.loads(line) for line in lines]\n",
    "    return data\n",
    "\n",
    "train_data = load_jsonl('./data/nikluge-iau-2023-train.jsonl')\n",
    "test_data = load_jsonl('./data/nikluge-iau-2023-dev.jsonl')\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'id': [item['id'] for item in train_data],\n",
    "    'text': [item['input'] for item in train_data],\n",
    "    'label': [1 if item['output'] == 'POSITIVE' else 0 for item in train_data]\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'id': [item['id'] for item in test_data],\n",
    "    'text': [item['input'] for item in test_data],\n",
    "    'label': [1 if item['output'] == 'POSITIVE' else 0 for item in test_data]\n",
    "})\n",
    "\n",
    "\n",
    "model_id = \"beomi/kcbert-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=8, lora_alpha=16, lora_dropout=0.1\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "\n",
    "    precision = precision_score(labels, preds, average='weighted')\n",
    "    recall = recall_score(labels, preds, average='weighted')\n",
    "    f1_macro = f1_score(labels, preds, average='macro')\n",
    "    f1_micro = f1_score(labels, preds, average='micro')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  num_train_epochs=10,\n",
    "                                  weight_decay=0.01,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  metric_for_best_model=\"f1_macro\",\n",
    "                                  greater_is_better=True,\n",
    "                                  )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# evalueate 결과를 저장\n",
    "result  = trainer.evaluate()\n",
    "\n",
    "# 결과를 score.csv에 저장\n",
    "with open('./data/score.csv', 'a') as file:\n",
    "    file.write(f'{model_id},{result[\"eval_accuracy\"]},{result[\"eval_precision\"]},{result[\"eval_recall\"]},{result[\"eval_f1_macro\"]},{result[\"eval_f1_micro\"]}\\n')\n",
    "\n",
    "\n",
    "# 테스트 데이터에 대한 예측 결과를 저장\n",
    "test_data = load_jsonl('./data/nikluge-iau-2023-test.jsonl')\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'id': [item['id'] for item in test_data],\n",
    "    'text': [item['input'] for item in test_data],\n",
    "})\n",
    "\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "\n",
    "output = []\n",
    "for idx, pred in enumerate(predictions.predictions):\n",
    "    output.append({\n",
    "        'id': test_data[idx]['id'],\n",
    "        'input': test_data[idx]['input'],\n",
    "        'output': 'POSITIVE' if pred.argmax() == 1 else 'NEGATIVE'\n",
    "    })\n",
    "    \n",
    "# JSONL 파일로 저장\n",
    "with open('./data/' + model_id.split('/')[1] + '-predictions.jsonl', 'w', encoding='utf-8') as file:\n",
    "    for item in output:\n",
    "        file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "!head -n 5 ./data/nikluge-iau-2023-test-predictions.jsonl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "18th",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
