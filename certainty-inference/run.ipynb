{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at kimcando/sbert-kornli-knoSTS-trained and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.16ba/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 37.49ba/s]\n",
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='787' max='1810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 787/1810 02:37 < 03:24, 5.00 it/s, Epoch 4.34/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.819159</td>\n",
       "      <td>1.819159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.520563</td>\n",
       "      <td>1.520563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.729300</td>\n",
       "      <td>1.452769</td>\n",
       "      <td>1.452770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.729300</td>\n",
       "      <td>1.655967</td>\n",
       "      <td>1.655967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 116\u001b[0m\n\u001b[1;32m    107\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    108\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    109\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# ëª¨ë¸ í•™ìŠµ\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# ëª¨ë¸ í‰ê°€\u001b[39;00m\n\u001b[1;32m    119\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/anaconda3/envs/18th/lib/python3.11/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/18th/lib/python3.11/site-packages/transformers/trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def load_custom_dataset(file_path):\n",
    "    import json\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['context'], examples['prompt'], truncation=True, padding='max_length', max_length=512)\n",
    "    inputs['label'] = [float(label) for label in examples['label']]  # ë ˆì´ë¸”ì„ float íƒ€ì…ìœ¼ë¡œ ë³€í™˜\n",
    "    return inputs\n",
    "\n",
    "# ìƒˆë¡œìš´ ë°ì´í„°ì…‹ ê²½ë¡œ\n",
    "train_data_path = './data/nikluge-2022-nli-train.jsonl'\n",
    "val_data_path = './data/nikluge-2022-nli-dev.jsonl'\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "train_data = load_custom_dataset(train_data_path)\n",
    "val_data = load_custom_dataset(val_data_path)\n",
    "\n",
    "# ë°ì´í„°ì…‹ í˜•ì‹ ë³€ê²½\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'context': [item['input']['context'] for item in train_data],\n",
    "    'prompt': [item['input']['prompt'] for item in train_data],\n",
    "    'label': [item['output'] for item in train_data]\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'context': [item['input']['context'] for item in val_data],\n",
    "    'prompt': [item['input']['prompt'] for item in val_data],\n",
    "    'label': [item['output'] for item in val_data]\n",
    "})\n",
    "\n",
    "\n",
    "# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "model_name = \"kimcando/sbert-kornli-knoSTS-trained\"\n",
    "# model_name = \"ys7yoo/sentence-roberta-large-kor-sts\"\n",
    "# model_name = \"Junmai/klue-roberta-large-copa-finetuned-v1\"\n",
    "# model_name = \"ddobokki/klue-roberta-base-nli-sts\"\n",
    "# model_name = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    "# model_name = \"jhgan/ko-sbert-multitask\"\n",
    "# model_name = \"BM-K/KoSimCSE-roberta-multitask\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "# ì–´ëŒ‘í„° ì„¤ì • ë° ì ìš©\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "# model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ì–‘ìí™” ì ìš©\n",
    "def apply_quantization(module):\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        return bnb.nn.Linear8bitLt(module.in_features, module.out_features, bias=module.bias is not None)\n",
    "    for name, child in module.named_children():\n",
    "        module.add_module(name, apply_quantization(child))\n",
    "    return module\n",
    "\n",
    "# model = apply_quantization(model)\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# ë ˆì´ë¸” ë°ì´í„° í˜•ì‹ ë³€í™˜\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# í•™ìŠµ ì„¤ì •\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_safetensors=False,  # safetensors ì‚¬ìš© ì•ˆí•¨\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"mse\",\n",
    ")\n",
    "\n",
    "# í‰ê°€ì§€í‘œ ì •ì˜\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.squeeze()\n",
    "    labels = p.label_ids.squeeze()\n",
    "    mse = ((preds - labels) ** 2).mean()\n",
    "    return {\"mse\": mse}\n",
    "\n",
    "# Trainer ì •ì˜\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# ëª¨ë¸ í•™ìŠµ\n",
    "trainer.train()\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€\n",
    "trainer.evaluate()\n",
    "\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ\n",
    "test_data_path = './data/nikluge-2022-nli-test.jsonl'\n",
    "test_data = load_custom_dataset(test_data_path)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í˜•ì‹ ë³€ê²½\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'id': [item['id'] for item in test_data],\n",
    "    'context': [item['input']['context'] for item in test_data],\n",
    "    'prompt': [item['input']['prompt'] for item in test_data],\n",
    "})\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ í•¨ìˆ˜ ì¬ì •ì˜\n",
    "def preprocess_test_function(examples):\n",
    "    inputs = tokenizer(examples['context'], examples['prompt'], truncation=True, padding='max_length', max_length=512)\n",
    "    return inputs\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_test_function, batched=True)\n",
    "\n",
    "# ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥\n",
    "output = []\n",
    "for idx, pred in enumerate(predictions.predictions):\n",
    "    output.append({\n",
    "        'id': test_data[idx]['id'],\n",
    "        'input': test_data[idx]['input'],\n",
    "        'output': float(pred[0])  # í™•ì‹ ì„± ì ìˆ˜ ì˜ˆì¸¡\n",
    "    })\n",
    "\n",
    "# JSONL íŒŒì¼ë¡œ ì €ì¥\n",
    "output_path = './data/certainty-predictions1.jsonl'\n",
    "with open(output_path, 'w', encoding='utf-8') as file:\n",
    "    for item in output:\n",
    "        file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# ì²« 5ì¤„ ì¶œë ¥\n",
    "!head -n 5 ./data/certainty-predictions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"nikluge-2022-nli-test-000001\", \"input\": {\"context\": \"ë˜ ì€í–‰ì˜ ì˜ˆÂ·ì ê¸ˆ ìƒí’ˆì€ ê°€ì…ì‹œì ì˜ ì•½ì •ì´ìœ¨ì´ ë§Œê¸°ê¹Œì§€ ì ìš©ë˜ì§€ë§Œ ì €ì¶•ì„±ë³´í—˜ì€ ê³µì‹œì´ìœ¨ ì ìš©ì£¼ê¸°ì— ë”°ë¼ ë³¸ì¸ì´ ê°€ì…í•œ ê³„ì•½ì˜ ì´ìœ¨ì´ ë³€ë™ë˜ë¯€ë¡œ í™˜ê¸‰ê¸ˆì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒì„ ê¸°ì–µí•´ì•¼ í•œë‹¤.\", \"prompt\": \"ì €ì¶•ì„± ë³´í—˜ì€ ê³µì‹œì´ìœ¨ ì ìš© ì£¼ê¸°ì— ë”°ë¼ ë³¸ì¸ì´ ê°€ì…í•œ ê³„ì•½ì˜ ì´ìœ¨ì´ ë³€ë™ë˜ë¯€ë¡œ í™˜ê¸‰ê¸ˆì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.\"}, \"output\": 6.259641647338867}\n",
      "{\"id\": \"nikluge-2022-nli-test-000002\", \"input\": {\"context\": \"ë°˜ë©´ â€˜ë‹¤ë¦¬ë‹¤â€™ëŠ” â€˜ì˜·ì´ë‚˜ ì²œ ë”°ìœ„ì˜ ì£¼ë¦„ì´ë‚˜ êµ¬ê¹€ì„ í´ê³  ì¤„ì„ ì„¸ìš°ê¸° ìœ„í•´ ë‹¤ë¦¬ë¯¸ë‚˜ ì¸ë‘ë¡œ ë¬¸ì§€ë¥¸ë‹¤â€™ë€ ëœ»ì´ë‹¤. ë”°ë¼ì„œ â€œë‹¤ë¦¬ë¯¸ë¡œ ì˜·ì„ ë‹¤ë¦¬ë‹¤â€ â€œë‹¤ë¦¬ì§€ ì•Šì€ ì™€ì´ì…”ì¸ ë¼ ì˜¨í†µ êµ¬ê¹€ì‚´ì´ ìˆë‹¤â€ì—ì„œ â€˜ë‹¤ë¦¬ë‹¤â€™ â€˜ë‹¤ë¦¬ì§€â€™ì²˜ëŸ¼ ì“´ë‹¤. ëª…ì‚¬ â€˜ë‹¤ë¦¬ë¯¸â€™ â€˜ë‹¤ë¦¼ì§ˆâ€™ì´ ë™ì‚¬ â€˜ë‹¤ë¦¬ë‹¤â€™ì—ì„œ íŒŒìƒëœ ë§ì´ë¼ê³  ê¸°ì–µí•˜ë©´ â€˜ë‹¬ì´ë‹¤â€™ì™€ â€˜ë‹¤ë¦¬ë‹¤â€™ì˜ êµ¬ë¶„ì´ ì¢€ ë” ì‰¬ìš¸ ë“¯í•˜ë‹¤.\", \"prompt\": \"ëª…ì‚¬ â€˜ë‹¤ë¦¬ë¯¸â€™ì™€ â€˜ë‹¤ë¦¼ì§ˆâ€™ì€ ë™ì‚¬ â€˜ë‹¤ë¦¬ë‹¤â€™ì—ì„œ íŒŒìƒëœ ë§ì´ë‹¤.\"}, \"output\": 6.223532199859619}\n",
      "{\"id\": \"nikluge-2022-nli-test-000003\", \"input\": {\"context\": \"ê°€ìŠ¤ê´€ ì‚¬ì—…ì—ì„œ ë‚¨ë¶ ê°„ ì‹ ë¢° ë¬¸ì œê°€ ê±¸ë¦¼ëŒì´ë¼ëŠ” ê²ƒì€, ê°€ìŠ¤ê´€ ì‚¬ì—…ì— ëŒ€í•œ ì§„ì§€í•œ ë…¼ì˜ëŠ” ë‚¨ë¶ ê°„ ì‹ ë¢°ë¥¼ ìŒ“ëŠ” ê¸°íšŒê°€ ë  ìˆ˜ ìˆë‹¤ëŠ” ëœ»ì´ê¸°ë„ í•˜ë‹¤. ë¶ì€ ê·¸ë™ì•ˆ ì–´ë–»ê²Œë“  ìš°ë¦¬ë¥¼ ê±´ë„ˆë›°ì–´ ë¯¸êµ­ê³¼ ì§ê±°ë˜í•´ë³´ë ¤ë‹¤ ë²ˆë²ˆì´ ì¢Œì ˆí–ˆë‹¤. ë¶ë„ ì´ì œëŠ” ì™¸ë¶€ ì„¸ê³„ì™€ ìˆ¨í†µì„ í‹”ìš°ë ¤ë©´ ë‚¨ë¶ ê´€ê³„ë¥¼ ì–´ëŠ ì •ë„ ì •ìƒ ê¶¤ë„ë¡œ ëŒë ¤ë†“ì§€ ì•Šìœ¼ë©´ ì•ˆ ëœë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•˜ì„ ê²ƒì´ë‹¤.\", \"prompt\": \"ë¶ì´ ì™¸ë¶€ ì„¸ê³„ì™€ ìˆ¨í†µì„ í‹”ìš°ë ¤ë©´, ë‚¨ë¶ ê´€ê³„ë¥¼ ì–´ëŠ ì •ë„ ì •ìƒ ê¶¤ë„ë¡œ ëŒë ¤ë†“ì§€ ì•Šìœ¼ë©´ ì•ˆ ëœë‹¤.\"}, \"output\": 6.282845973968506}\n",
      "{\"id\": \"nikluge-2022-nli-test-000004\", \"input\": {\"context\": \"ì´ ë•Œë¬¸ì— ë¬´ì˜ì‹ì ì¸ ë‡ŒëŠ” ì†ë„ëŠ” ë¹ ë¥´ì§€ë§Œ ì •êµí•¨ì´ ë–¨ì–´ì§„ë‹¤. ê·¸ë˜ì„œ ìì£¼ ì‹¤ìˆ˜ë¥¼ ì €ì§€ë¥¸ë‹¤. ë¬¸ì¥ì„ ì½ì„ ë•Œ ê·¸ ëœ»ì€ ì´í•´í–ˆì§€ë§Œ ì² ìê°€ í‹€ë¦° ê²ƒì„ ë°œê²¬í•˜ì§€ ëª»í–ˆë‹¤ë©´ ê·¸ê²ƒì€ ë¬´ì˜ì‹ì ì¸ ë‡Œê°€ ì¼ì„ ë¹ ë¥´ê²Œ ì²˜ë¦¬í–ˆê¸° ë•Œë¬¸ì´ë‹¤.\", \"prompt\": \"ë¬¸ì¥ì˜ ì² ìê°€ í‹€ë ¸ë‹¤.\"}, \"output\": 6.207643032073975}\n",
      "{\"id\": \"nikluge-2022-nli-test-000005\", \"input\": {\"context\": \"ì¤‘êµ­ëŒ€ì‚¬ê´€ì´ â€œê²€ì‚¬ ì¸¡ì—ì„œ ì œì¶œí•œ ê³µë¬¸ ë“± 3ê±´ì˜ ë¬¸ì„œëŠ” ëª¨ë‘ ìœ„ì¡°ëœ ê²ƒâ€ì´ë¼ê³  ë°í˜”ê³ , ëŒ€í•œë¯¼êµ­ ì™¸êµë¶€ ìˆ˜ì¥ì´ â€˜ê³µë¬¸ì„œ 2ê±´ì€ ì •ì‹ ì™¸êµê²½ë¡œë¥¼ í†µí•˜ì§€ ì•Šì•˜ë‹¤â€™ê³  ë°í˜”ìœ¼ë©´ ì‚¬ì‹¤ìƒ â€˜ìƒí™©ì€ ì¢…ë£Œëœ ê²ƒâ€™ìœ¼ë¡œ ë´ë„ ëœë‹¤.\", \"prompt\": \"ê³µë¬¸ì„œ 2ê±´ì€ ì •ì‹ ì™¸êµ ê²½ë¡œë¥¼ í†µí•˜ì§€ ì•Šì•˜ë‹¤.\"}, \"output\": 6.1461663246154785}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# JSONL íŒŒì¼ë¡œ ì €ì¥\n",
    "output_path = './data/ci.jsonl'\n",
    "with open(output_path, 'w', encoding='utf-8') as file:\n",
    "    for item in output:\n",
    "        file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# ì²« 5ì¤„ ì¶œë ¥\n",
    "!head -n 5 ./data/certainty-predictions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"nikluge-2022-nli-test-000001\", \"input\": {\"context\": \"ë˜ ì€í–‰ì˜ ì˜ˆÂ·ì ê¸ˆ ìƒí’ˆì€ ê°€ì…ì‹œì ì˜ ì•½ì •ì´ìœ¨ì´ ë§Œê¸°ê¹Œì§€ ì ìš©ë˜ì§€ë§Œ ì €ì¶•ì„±ë³´í—˜ì€ ê³µì‹œì´ìœ¨ ì ìš©ì£¼ê¸°ì— ë”°ë¼ ë³¸ì¸ì´ ê°€ì…í•œ ê³„ì•½ì˜ ì´ìœ¨ì´ ë³€ë™ë˜ë¯€ë¡œ í™˜ê¸‰ê¸ˆì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒì„ ê¸°ì–µí•´ì•¼ í•œë‹¤.\", \"prompt\": \"ì €ì¶•ì„± ë³´í—˜ì€ ê³µì‹œì´ìœ¨ ì ìš© ì£¼ê¸°ì— ë”°ë¼ ë³¸ì¸ì´ ê°€ì…í•œ ê³„ì•½ì˜ ì´ìœ¨ì´ ë³€ë™ë˜ë¯€ë¡œ í™˜ê¸‰ê¸ˆì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤.\"}, \"output\": 6.259641647338867}\n",
      "{\"id\": \"nikluge-2022-nli-test-000002\", \"input\": {\"context\": \"ë°˜ë©´ â€˜ë‹¤ë¦¬ë‹¤â€™ëŠ” â€˜ì˜·ì´ë‚˜ ì²œ ë”°ìœ„ì˜ ì£¼ë¦„ì´ë‚˜ êµ¬ê¹€ì„ í´ê³  ì¤„ì„ ì„¸ìš°ê¸° ìœ„í•´ ë‹¤ë¦¬ë¯¸ë‚˜ ì¸ë‘ë¡œ ë¬¸ì§€ë¥¸ë‹¤â€™ë€ ëœ»ì´ë‹¤. ë”°ë¼ì„œ â€œë‹¤ë¦¬ë¯¸ë¡œ ì˜·ì„ ë‹¤ë¦¬ë‹¤â€ â€œë‹¤ë¦¬ì§€ ì•Šì€ ì™€ì´ì…”ì¸ ë¼ ì˜¨í†µ êµ¬ê¹€ì‚´ì´ ìˆë‹¤â€ì—ì„œ â€˜ë‹¤ë¦¬ë‹¤â€™ â€˜ë‹¤ë¦¬ì§€â€™ì²˜ëŸ¼ ì“´ë‹¤. ëª…ì‚¬ â€˜ë‹¤ë¦¬ë¯¸â€™ â€˜ë‹¤ë¦¼ì§ˆâ€™ì´ ë™ì‚¬ â€˜ë‹¤ë¦¬ë‹¤â€™ì—ì„œ íŒŒìƒëœ ë§ì´ë¼ê³  ê¸°ì–µí•˜ë©´ â€˜ë‹¬ì´ë‹¤â€™ì™€ â€˜ë‹¤ë¦¬ë‹¤â€™ì˜ êµ¬ë¶„ì´ ì¢€ ë” ì‰¬ìš¸ ë“¯í•˜ë‹¤.\", \"prompt\": \"ëª…ì‚¬ â€˜ë‹¤ë¦¬ë¯¸â€™ì™€ â€˜ë‹¤ë¦¼ì§ˆâ€™ì€ ë™ì‚¬ â€˜ë‹¤ë¦¬ë‹¤â€™ì—ì„œ íŒŒìƒëœ ë§ì´ë‹¤.\"}, \"output\": 6.223532199859619}\n",
      "{\"id\": \"nikluge-2022-nli-test-000003\", \"input\": {\"context\": \"ê°€ìŠ¤ê´€ ì‚¬ì—…ì—ì„œ ë‚¨ë¶ ê°„ ì‹ ë¢° ë¬¸ì œê°€ ê±¸ë¦¼ëŒì´ë¼ëŠ” ê²ƒì€, ê°€ìŠ¤ê´€ ì‚¬ì—…ì— ëŒ€í•œ ì§„ì§€í•œ ë…¼ì˜ëŠ” ë‚¨ë¶ ê°„ ì‹ ë¢°ë¥¼ ìŒ“ëŠ” ê¸°íšŒê°€ ë  ìˆ˜ ìˆë‹¤ëŠ” ëœ»ì´ê¸°ë„ í•˜ë‹¤. ë¶ì€ ê·¸ë™ì•ˆ ì–´ë–»ê²Œë“  ìš°ë¦¬ë¥¼ ê±´ë„ˆë›°ì–´ ë¯¸êµ­ê³¼ ì§ê±°ë˜í•´ë³´ë ¤ë‹¤ ë²ˆë²ˆì´ ì¢Œì ˆí–ˆë‹¤. ë¶ë„ ì´ì œëŠ” ì™¸ë¶€ ì„¸ê³„ì™€ ìˆ¨í†µì„ í‹”ìš°ë ¤ë©´ ë‚¨ë¶ ê´€ê³„ë¥¼ ì–´ëŠ ì •ë„ ì •ìƒ ê¶¤ë„ë¡œ ëŒë ¤ë†“ì§€ ì•Šìœ¼ë©´ ì•ˆ ëœë‹¤ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•˜ì„ ê²ƒì´ë‹¤.\", \"prompt\": \"ë¶ì´ ì™¸ë¶€ ì„¸ê³„ì™€ ìˆ¨í†µì„ í‹”ìš°ë ¤ë©´, ë‚¨ë¶ ê´€ê³„ë¥¼ ì–´ëŠ ì •ë„ ì •ìƒ ê¶¤ë„ë¡œ ëŒë ¤ë†“ì§€ ì•Šìœ¼ë©´ ì•ˆ ëœë‹¤.\"}, \"output\": 6.282845973968506}\n",
      "{\"id\": \"nikluge-2022-nli-test-000004\", \"input\": {\"context\": \"ì´ ë•Œë¬¸ì— ë¬´ì˜ì‹ì ì¸ ë‡ŒëŠ” ì†ë„ëŠ” ë¹ ë¥´ì§€ë§Œ ì •êµí•¨ì´ ë–¨ì–´ì§„ë‹¤. ê·¸ë˜ì„œ ìì£¼ ì‹¤ìˆ˜ë¥¼ ì €ì§€ë¥¸ë‹¤. ë¬¸ì¥ì„ ì½ì„ ë•Œ ê·¸ ëœ»ì€ ì´í•´í–ˆì§€ë§Œ ì² ìê°€ í‹€ë¦° ê²ƒì„ ë°œê²¬í•˜ì§€ ëª»í–ˆë‹¤ë©´ ê·¸ê²ƒì€ ë¬´ì˜ì‹ì ì¸ ë‡Œê°€ ì¼ì„ ë¹ ë¥´ê²Œ ì²˜ë¦¬í–ˆê¸° ë•Œë¬¸ì´ë‹¤.\", \"prompt\": \"ë¬¸ì¥ì˜ ì² ìê°€ í‹€ë ¸ë‹¤.\"}, \"output\": 6.207643032073975}\n",
      "{\"id\": \"nikluge-2022-nli-test-000005\", \"input\": {\"context\": \"ì¤‘êµ­ëŒ€ì‚¬ê´€ì´ â€œê²€ì‚¬ ì¸¡ì—ì„œ ì œì¶œí•œ ê³µë¬¸ ë“± 3ê±´ì˜ ë¬¸ì„œëŠ” ëª¨ë‘ ìœ„ì¡°ëœ ê²ƒâ€ì´ë¼ê³  ë°í˜”ê³ , ëŒ€í•œë¯¼êµ­ ì™¸êµë¶€ ìˆ˜ì¥ì´ â€˜ê³µë¬¸ì„œ 2ê±´ì€ ì •ì‹ ì™¸êµê²½ë¡œë¥¼ í†µí•˜ì§€ ì•Šì•˜ë‹¤â€™ê³  ë°í˜”ìœ¼ë©´ ì‚¬ì‹¤ìƒ â€˜ìƒí™©ì€ ì¢…ë£Œëœ ê²ƒâ€™ìœ¼ë¡œ ë´ë„ ëœë‹¤.\", \"prompt\": \"ê³µë¬¸ì„œ 2ê±´ì€ ì •ì‹ ì™¸êµ ê²½ë¡œë¥¼ í†µí•˜ì§€ ì•Šì•˜ë‹¤.\"}, \"output\": 6.1461663246154785}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# JSONL íŒŒì¼ë¡œ ì €ì¥\n",
    "output_path = './data/certainty-predictions-fin.jsonl'\n",
    "with open(output_path, 'w', encoding='utf-8') as file:\n",
    "    for item in output:\n",
    "        file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# ì²« 5ì¤„ ì¶œë ¥\n",
    "!head -n 5 ./data/certainty-predictions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   3472 MiB |  12970 MiB |  67702 GiB |  67699 GiB |\n",
      "|       from large pool |   3454 MiB |  12952 MiB |  67490 GiB |  67486 GiB |\n",
      "|       from small pool |     17 MiB |     23 MiB |    212 GiB |    212 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   3472 MiB |  12970 MiB |  67702 GiB |  67699 GiB |\n",
      "|       from large pool |   3454 MiB |  12952 MiB |  67490 GiB |  67486 GiB |\n",
      "|       from small pool |     17 MiB |     23 MiB |    212 GiB |    212 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   3471 MiB |  12969 MiB |  67702 GiB |  67698 GiB |\n",
      "|       from large pool |   3453 MiB |  12951 MiB |  67490 GiB |  67486 GiB |\n",
      "|       from small pool |     17 MiB |     23 MiB |    212 GiB |    212 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   3498 MiB |  13314 MiB | 477294 MiB | 473796 MiB |\n",
      "|       from large pool |   3480 MiB |  13294 MiB | 474904 MiB | 471424 MiB |\n",
      "|       from small pool |     18 MiB |     24 MiB |   2390 MiB |   2372 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  26176 KiB | 384437 KiB |  23641 GiB |  23641 GiB |\n",
      "|       from large pool |  25768 KiB | 382712 KiB |  23420 GiB |  23420 GiB |\n",
      "|       from small pool |    408 KiB |   7038 KiB |    221 GiB |    221 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1298    |    1652    |    2791 K  |    2790 K  |\n",
      "|       from large pool |     301    |     552    |    1937 K  |    1937 K  |\n",
      "|       from small pool |     997    |    1100    |     853 K  |     852 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1298    |    1652    |    2791 K  |    2790 K  |\n",
      "|       from large pool |     301    |     552    |    1937 K  |    1937 K  |\n",
      "|       from small pool |     997    |    1100    |     853 K  |     852 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     140    |     294    |    6380    |    6240    |\n",
      "|       from large pool |     131    |     284    |    5185    |    5054    |\n",
      "|       from small pool |       9    |      12    |    1195    |    1186    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      13    |      36    |    1525 K  |    1525 K  |\n",
      "|       from large pool |       5    |      13    |    1176 K  |    1176 K  |\n",
      "|       from small pool |       8    |      30    |     348 K  |     348 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "# stringì„ 20ì¤„ì”© ëŠì–´ì„œ ì¶œë ¥\n",
    "string = string.split('\\n')\n",
    "for i in range(0, len(string), 20):\n",
    "    print('\\n'.join(string[i:i+20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "18th",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
