{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at kimcando/sbert-kornli-knoSTS-trained and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00, 10.16ba/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 37.49ba/s]\n",
      "/home/link/anaconda3/envs/18th/lib/python3.11/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='787' max='1810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 787/1810 02:37 < 03:24, 5.00 it/s, Epoch 4.34/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.819159</td>\n",
       "      <td>1.819159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.520563</td>\n",
       "      <td>1.520563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.729300</td>\n",
       "      <td>1.452769</td>\n",
       "      <td>1.452770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.729300</td>\n",
       "      <td>1.655967</td>\n",
       "      <td>1.655967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 116\u001b[0m\n\u001b[1;32m    107\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    108\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    109\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Î™®Îç∏ ÌïôÏäµ\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m# Î™®Îç∏ ÌèâÍ∞Ä\u001b[39;00m\n\u001b[1;32m    119\u001b[0m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/anaconda3/envs/18th/lib/python3.11/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/18th/lib/python3.11/site-packages/transformers/trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_metric\n",
    "from transformers import BitsAndBytesConfig\n",
    "import bitsandbytes as bnb\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò\n",
    "def load_custom_dataset(file_path):\n",
    "    import json\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['context'], examples['prompt'], truncation=True, padding='max_length', max_length=512)\n",
    "    inputs['label'] = [float(label) for label in examples['label']]  # Î†àÏù¥Î∏îÏùÑ float ÌÉÄÏûÖÏúºÎ°ú Î≥ÄÌôò\n",
    "    return inputs\n",
    "\n",
    "# ÏÉàÎ°úÏö¥ Îç∞Ïù¥ÌÑ∞ÏÖã Í≤ΩÎ°ú\n",
    "train_data_path = './data/nikluge-2022-nli-train.jsonl'\n",
    "val_data_path = './data/nikluge-2022-nli-dev.jsonl'\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "train_data = load_custom_dataset(train_data_path)\n",
    "val_data = load_custom_dataset(val_data_path)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã ÌòïÏãù Î≥ÄÍ≤Ω\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'context': [item['input']['context'] for item in train_data],\n",
    "    'prompt': [item['input']['prompt'] for item in train_data],\n",
    "    'label': [item['output'] for item in train_data]\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'context': [item['input']['context'] for item in val_data],\n",
    "    'prompt': [item['input']['prompt'] for item in val_data],\n",
    "    'label': [item['output'] for item in val_data]\n",
    "})\n",
    "\n",
    "\n",
    "# Î™®Îç∏Í≥º ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú\n",
    "model_name = \"kimcando/sbert-kornli-knoSTS-trained\"\n",
    "# model_name = \"ys7yoo/sentence-roberta-large-kor-sts\"\n",
    "# model_name = \"Junmai/klue-roberta-large-copa-finetuned-v1\"\n",
    "# model_name = \"ddobokki/klue-roberta-base-nli-sts\"\n",
    "# model_name = \"snunlp/KR-SBERT-V40K-klueNLI-augSTS\"\n",
    "# model_name = \"jhgan/ko-sbert-multitask\"\n",
    "# model_name = \"BM-K/KoSimCSE-roberta-multitask\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "# Ïñ¥ÎåëÌÑ∞ ÏÑ§Ï†ï Î∞è Ï†ÅÏö©\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "# model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ÏñëÏûêÌôî Ï†ÅÏö©\n",
    "def apply_quantization(module):\n",
    "    if isinstance(module, torch.nn.Linear):\n",
    "        return bnb.nn.Linear8bitLt(module.in_features, module.out_features, bias=module.bias is not None)\n",
    "    for name, child in module.named_children():\n",
    "        module.add_module(name, apply_quantization(child))\n",
    "    return module\n",
    "\n",
    "# model = apply_quantization(model)\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ÑÏ≤òÎ¶¨\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Î†àÏù¥Î∏î Îç∞Ïù¥ÌÑ∞ ÌòïÏãù Î≥ÄÌôò\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# ÌïôÏäµ ÏÑ§Ï†ï\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_safetensors=False,  # safetensors ÏÇ¨Ïö© ÏïàÌï®\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    metric_for_best_model=\"mse\",\n",
    ")\n",
    "\n",
    "# ÌèâÍ∞ÄÏßÄÌëú Ï†ïÏùò\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.squeeze()\n",
    "    labels = p.label_ids.squeeze()\n",
    "    mse = ((preds - labels) ** 2).mean()\n",
    "    return {\"mse\": mse}\n",
    "\n",
    "# Trainer Ï†ïÏùò\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "# Î™®Îç∏ ÌïôÏäµ\n",
    "trainer.train()\n",
    "\n",
    "# Î™®Îç∏ ÌèâÍ∞Ä\n",
    "trainer.evaluate()\n",
    "\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "test_data_path = './data/nikluge-2022-nli-test.jsonl'\n",
    "test_data = load_custom_dataset(test_data_path)\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖã ÌòïÏãù Î≥ÄÍ≤Ω\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'id': [item['id'] for item in test_data],\n",
    "    'context': [item['input']['context'] for item in test_data],\n",
    "    'prompt': [item['input']['prompt'] for item in test_data],\n",
    "})\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò Ïû¨Ï†ïÏùò\n",
    "def preprocess_test_function(examples):\n",
    "    inputs = tokenizer(examples['context'], examples['prompt'], truncation=True, padding='max_length', max_length=512)\n",
    "    return inputs\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã Ï†ÑÏ≤òÎ¶¨\n",
    "tokenized_test_dataset = test_dataset.map(preprocess_test_function, batched=True)\n",
    "\n",
    "# ÏòàÏ∏° ÏàòÌñâ\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "\n",
    "# Í≤∞Í≥º Ï†ÄÏû•\n",
    "output = []\n",
    "for idx, pred in enumerate(predictions.predictions):\n",
    "    output.append({\n",
    "        'id': test_data[idx]['id'],\n",
    "        'input': test_data[idx]['input'],\n",
    "        'output': float(pred[0])  # ÌôïÏã†ÏÑ± Ï†êÏàò ÏòàÏ∏°\n",
    "    })\n",
    "\n",
    "# JSONL ÌååÏùºÎ°ú Ï†ÄÏû•\n",
    "output_path = './data/certainty-predictions1.jsonl'\n",
    "with open(output_path, 'w', encoding='utf-8') as file:\n",
    "    for item in output:\n",
    "        file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Ï≤´ 5Ï§Ñ Ï∂úÎ†•\n",
    "!head -n 5 ./data/certainty-predictions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"nikluge-2022-nli-test-000001\", \"input\": {\"context\": \"Îòê ÏùÄÌñâÏùò Ïòà¬∑Ï†ÅÍ∏à ÏÉÅÌíàÏùÄ Í∞ÄÏûÖÏãúÏ†êÏùò ÏïΩÏ†ïÏù¥Ïú®Ïù¥ ÎßåÍ∏∞ÍπåÏßÄ Ï†ÅÏö©ÎêòÏßÄÎßå Ï†ÄÏ∂ïÏÑ±Î≥¥ÌóòÏùÄ Í≥µÏãúÏù¥Ïú® Ï†ÅÏö©Ï£ºÍ∏∞Ïóê Îî∞Îùº Î≥∏Ïù∏Ïù¥ Í∞ÄÏûÖÌïú Í≥ÑÏïΩÏùò Ïù¥Ïú®Ïù¥ Î≥ÄÎèôÎêòÎØÄÎ°ú ÌôòÍ∏âÍ∏àÏù¥ Îã¨ÎùºÏßà Ïàò ÏûàÏùåÏùÑ Í∏∞ÏñµÌï¥Ïïº ÌïúÎã§.\", \"prompt\": \"Ï†ÄÏ∂ïÏÑ± Î≥¥ÌóòÏùÄ Í≥µÏãúÏù¥Ïú® Ï†ÅÏö© Ï£ºÍ∏∞Ïóê Îî∞Îùº Î≥∏Ïù∏Ïù¥ Í∞ÄÏûÖÌïú Í≥ÑÏïΩÏùò Ïù¥Ïú®Ïù¥ Î≥ÄÎèôÎêòÎØÄÎ°ú ÌôòÍ∏âÍ∏àÏù¥ Îã¨ÎùºÏßà Ïàò ÏûàÎã§.\"}, \"output\": 6.259641647338867}\n",
      "{\"id\": \"nikluge-2022-nli-test-000002\", \"input\": {\"context\": \"Î∞òÎ©¥ ‚ÄòÎã§Î¶¨Îã§‚ÄôÎäî ‚ÄòÏò∑Ïù¥ÎÇò Ï≤ú Îî∞ÏúÑÏùò Ï£ºÎ¶ÑÏù¥ÎÇò Íµ¨ÍπÄÏùÑ Ìé¥Í≥† Ï§ÑÏùÑ ÏÑ∏Ïö∞Í∏∞ ÏúÑÌï¥ Îã§Î¶¨ÎØ∏ÎÇò Ïù∏ÎëêÎ°ú Î¨∏ÏßÄÎ•∏Îã§‚ÄôÎûÄ ÎúªÏù¥Îã§. Îî∞ÎùºÏÑú ‚ÄúÎã§Î¶¨ÎØ∏Î°ú Ïò∑ÏùÑ Îã§Î¶¨Îã§‚Äù ‚ÄúÎã§Î¶¨ÏßÄ ÏïäÏùÄ ÏôÄÏù¥ÏÖîÏ∏†Îùº Ïò®ÌÜµ Íµ¨ÍπÄÏÇ¥Ïù¥ ÏûàÎã§‚ÄùÏóêÏÑú ‚ÄòÎã§Î¶¨Îã§‚Äô ‚ÄòÎã§Î¶¨ÏßÄ‚ÄôÏ≤òÎüº Ïì¥Îã§. Î™ÖÏÇ¨ ‚ÄòÎã§Î¶¨ÎØ∏‚Äô ‚ÄòÎã§Î¶ºÏßà‚ÄôÏù¥ ÎèôÏÇ¨ ‚ÄòÎã§Î¶¨Îã§‚ÄôÏóêÏÑú ÌååÏÉùÎêú ÎßêÏù¥ÎùºÍ≥† Í∏∞ÏñµÌïòÎ©¥ ‚ÄòÎã¨Ïù¥Îã§‚ÄôÏôÄ ‚ÄòÎã§Î¶¨Îã§‚ÄôÏùò Íµ¨Î∂ÑÏù¥ Ï¢Ä Îçî Ïâ¨Ïö∏ ÎìØÌïòÎã§.\", \"prompt\": \"Î™ÖÏÇ¨ ‚ÄòÎã§Î¶¨ÎØ∏‚ÄôÏôÄ ‚ÄòÎã§Î¶ºÏßà‚ÄôÏùÄ ÎèôÏÇ¨ ‚ÄòÎã§Î¶¨Îã§‚ÄôÏóêÏÑú ÌååÏÉùÎêú ÎßêÏù¥Îã§.\"}, \"output\": 6.223532199859619}\n",
      "{\"id\": \"nikluge-2022-nli-test-000003\", \"input\": {\"context\": \"Í∞ÄÏä§Í¥Ä ÏÇ¨ÏóÖÏóêÏÑú ÎÇ®Î∂Å Í∞Ñ Ïã†Î¢∞ Î¨∏Ï†úÍ∞Ä Í±∏Î¶ºÎèåÏù¥ÎùºÎäî Í≤ÉÏùÄ, Í∞ÄÏä§Í¥Ä ÏÇ¨ÏóÖÏóê ÎåÄÌïú ÏßÑÏßÄÌïú ÎÖºÏùòÎäî ÎÇ®Î∂Å Í∞Ñ Ïã†Î¢∞Î•º ÏåìÎäî Í∏∞ÌöåÍ∞Ä Îê† Ïàò ÏûàÎã§Îäî ÎúªÏù¥Í∏∞ÎèÑ ÌïòÎã§. Î∂ÅÏùÄ Í∑∏ÎèôÏïà Ïñ¥ÎñªÍ≤åÎì† Ïö∞Î¶¨Î•º Í±¥ÎÑàÎõ∞Ïñ¥ ÎØ∏Íµ≠Í≥º ÏßÅÍ±∞ÎûòÌï¥Î≥¥Î†§Îã§ Î≤àÎ≤àÏù¥ Ï¢åÏ†àÌñàÎã§. Î∂ÅÎèÑ Ïù¥Ï†úÎäî Ïô∏Î∂Ä ÏÑ∏Í≥ÑÏôÄ Ïà®ÌÜµÏùÑ ÌãîÏö∞Î†§Î©¥ ÎÇ®Î∂Å Í¥ÄÍ≥ÑÎ•º Ïñ¥Îäê Ï†ïÎèÑ Ï†ïÏÉÅ Í∂§ÎèÑÎ°ú ÎèåÎ†§ÎÜìÏßÄ ÏïäÏúºÎ©¥ Ïïà ÎêúÎã§Îäî Í≤ÉÏùÑ Íπ®Îã¨ÏïòÏùÑ Í≤ÉÏù¥Îã§.\", \"prompt\": \"Î∂ÅÏù¥ Ïô∏Î∂Ä ÏÑ∏Í≥ÑÏôÄ Ïà®ÌÜµÏùÑ ÌãîÏö∞Î†§Î©¥, ÎÇ®Î∂Å Í¥ÄÍ≥ÑÎ•º Ïñ¥Îäê Ï†ïÎèÑ Ï†ïÏÉÅ Í∂§ÎèÑÎ°ú ÎèåÎ†§ÎÜìÏßÄ ÏïäÏúºÎ©¥ Ïïà ÎêúÎã§.\"}, \"output\": 6.282845973968506}\n",
      "{\"id\": \"nikluge-2022-nli-test-000004\", \"input\": {\"context\": \"Ïù¥ ÎïåÎ¨∏Ïóê Î¨¥ÏùòÏãùÏ†ÅÏù∏ ÎáåÎäî ÏÜçÎèÑÎäî Îπ†Î•¥ÏßÄÎßå Ï†ïÍµêÌï®Ïù¥ Îñ®Ïñ¥ÏßÑÎã§. Í∑∏ÎûòÏÑú ÏûêÏ£º Ïã§ÏàòÎ•º Ï†ÄÏßÄÎ•∏Îã§. Î¨∏Ïû•ÏùÑ ÏùΩÏùÑ Îïå Í∑∏ ÎúªÏùÄ Ïù¥Ìï¥ÌñàÏßÄÎßå Ï≤†ÏûêÍ∞Ä ÌãÄÎ¶∞ Í≤ÉÏùÑ Î∞úÍ≤¨ÌïòÏßÄ Î™ªÌñàÎã§Î©¥ Í∑∏Í≤ÉÏùÄ Î¨¥ÏùòÏãùÏ†ÅÏù∏ ÎáåÍ∞Ä ÏùºÏùÑ Îπ†Î•¥Í≤å Ï≤òÎ¶¨ÌñàÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§.\", \"prompt\": \"Î¨∏Ïû•Ïùò Ï≤†ÏûêÍ∞Ä ÌãÄÎ†∏Îã§.\"}, \"output\": 6.207643032073975}\n",
      "{\"id\": \"nikluge-2022-nli-test-000005\", \"input\": {\"context\": \"Ï§ëÍµ≠ÎåÄÏÇ¨Í¥ÄÏù¥ ‚ÄúÍ≤ÄÏÇ¨ Ï∏°ÏóêÏÑú Ï†úÏ∂úÌïú Í≥µÎ¨∏ Îì± 3Í±¥Ïùò Î¨∏ÏÑúÎäî Î™®Îëê ÏúÑÏ°∞Îêú Í≤É‚ÄùÏù¥ÎùºÍ≥† Î∞ùÌòîÍ≥†, ÎåÄÌïúÎØºÍµ≠ Ïô∏ÍµêÎ∂Ä ÏàòÏû•Ïù¥ ‚ÄòÍ≥µÎ¨∏ÏÑú 2Í±¥ÏùÄ Ï†ïÏãù Ïô∏ÍµêÍ≤ΩÎ°úÎ•º ÌÜµÌïòÏßÄ ÏïäÏïòÎã§‚ÄôÍ≥† Î∞ùÌòîÏúºÎ©¥ ÏÇ¨Ïã§ÏÉÅ ‚ÄòÏÉÅÌô©ÏùÄ Ï¢ÖÎ£åÎêú Í≤É‚ÄôÏúºÎ°ú Î¥êÎèÑ ÎêúÎã§.\", \"prompt\": \"Í≥µÎ¨∏ÏÑú 2Í±¥ÏùÄ Ï†ïÏãù Ïô∏Íµê Í≤ΩÎ°úÎ•º ÌÜµÌïòÏßÄ ÏïäÏïòÎã§.\"}, \"output\": 6.1461663246154785}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# JSONL ÌååÏùºÎ°ú Ï†ÄÏû•\n",
    "output_path = './data/ci.jsonl'\n",
    "with open(output_path, 'w', encoding='utf-8') as file:\n",
    "    for item in output:\n",
    "        file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Ï≤´ 5Ï§Ñ Ï∂úÎ†•\n",
    "!head -n 5 ./data/certainty-predictions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"nikluge-2022-nli-test-000001\", \"input\": {\"context\": \"Îòê ÏùÄÌñâÏùò Ïòà¬∑Ï†ÅÍ∏à ÏÉÅÌíàÏùÄ Í∞ÄÏûÖÏãúÏ†êÏùò ÏïΩÏ†ïÏù¥Ïú®Ïù¥ ÎßåÍ∏∞ÍπåÏßÄ Ï†ÅÏö©ÎêòÏßÄÎßå Ï†ÄÏ∂ïÏÑ±Î≥¥ÌóòÏùÄ Í≥µÏãúÏù¥Ïú® Ï†ÅÏö©Ï£ºÍ∏∞Ïóê Îî∞Îùº Î≥∏Ïù∏Ïù¥ Í∞ÄÏûÖÌïú Í≥ÑÏïΩÏùò Ïù¥Ïú®Ïù¥ Î≥ÄÎèôÎêòÎØÄÎ°ú ÌôòÍ∏âÍ∏àÏù¥ Îã¨ÎùºÏßà Ïàò ÏûàÏùåÏùÑ Í∏∞ÏñµÌï¥Ïïº ÌïúÎã§.\", \"prompt\": \"Ï†ÄÏ∂ïÏÑ± Î≥¥ÌóòÏùÄ Í≥µÏãúÏù¥Ïú® Ï†ÅÏö© Ï£ºÍ∏∞Ïóê Îî∞Îùº Î≥∏Ïù∏Ïù¥ Í∞ÄÏûÖÌïú Í≥ÑÏïΩÏùò Ïù¥Ïú®Ïù¥ Î≥ÄÎèôÎêòÎØÄÎ°ú ÌôòÍ∏âÍ∏àÏù¥ Îã¨ÎùºÏßà Ïàò ÏûàÎã§.\"}, \"output\": 6.259641647338867}\n",
      "{\"id\": \"nikluge-2022-nli-test-000002\", \"input\": {\"context\": \"Î∞òÎ©¥ ‚ÄòÎã§Î¶¨Îã§‚ÄôÎäî ‚ÄòÏò∑Ïù¥ÎÇò Ï≤ú Îî∞ÏúÑÏùò Ï£ºÎ¶ÑÏù¥ÎÇò Íµ¨ÍπÄÏùÑ Ìé¥Í≥† Ï§ÑÏùÑ ÏÑ∏Ïö∞Í∏∞ ÏúÑÌï¥ Îã§Î¶¨ÎØ∏ÎÇò Ïù∏ÎëêÎ°ú Î¨∏ÏßÄÎ•∏Îã§‚ÄôÎûÄ ÎúªÏù¥Îã§. Îî∞ÎùºÏÑú ‚ÄúÎã§Î¶¨ÎØ∏Î°ú Ïò∑ÏùÑ Îã§Î¶¨Îã§‚Äù ‚ÄúÎã§Î¶¨ÏßÄ ÏïäÏùÄ ÏôÄÏù¥ÏÖîÏ∏†Îùº Ïò®ÌÜµ Íµ¨ÍπÄÏÇ¥Ïù¥ ÏûàÎã§‚ÄùÏóêÏÑú ‚ÄòÎã§Î¶¨Îã§‚Äô ‚ÄòÎã§Î¶¨ÏßÄ‚ÄôÏ≤òÎüº Ïì¥Îã§. Î™ÖÏÇ¨ ‚ÄòÎã§Î¶¨ÎØ∏‚Äô ‚ÄòÎã§Î¶ºÏßà‚ÄôÏù¥ ÎèôÏÇ¨ ‚ÄòÎã§Î¶¨Îã§‚ÄôÏóêÏÑú ÌååÏÉùÎêú ÎßêÏù¥ÎùºÍ≥† Í∏∞ÏñµÌïòÎ©¥ ‚ÄòÎã¨Ïù¥Îã§‚ÄôÏôÄ ‚ÄòÎã§Î¶¨Îã§‚ÄôÏùò Íµ¨Î∂ÑÏù¥ Ï¢Ä Îçî Ïâ¨Ïö∏ ÎìØÌïòÎã§.\", \"prompt\": \"Î™ÖÏÇ¨ ‚ÄòÎã§Î¶¨ÎØ∏‚ÄôÏôÄ ‚ÄòÎã§Î¶ºÏßà‚ÄôÏùÄ ÎèôÏÇ¨ ‚ÄòÎã§Î¶¨Îã§‚ÄôÏóêÏÑú ÌååÏÉùÎêú ÎßêÏù¥Îã§.\"}, \"output\": 6.223532199859619}\n",
      "{\"id\": \"nikluge-2022-nli-test-000003\", \"input\": {\"context\": \"Í∞ÄÏä§Í¥Ä ÏÇ¨ÏóÖÏóêÏÑú ÎÇ®Î∂Å Í∞Ñ Ïã†Î¢∞ Î¨∏Ï†úÍ∞Ä Í±∏Î¶ºÎèåÏù¥ÎùºÎäî Í≤ÉÏùÄ, Í∞ÄÏä§Í¥Ä ÏÇ¨ÏóÖÏóê ÎåÄÌïú ÏßÑÏßÄÌïú ÎÖºÏùòÎäî ÎÇ®Î∂Å Í∞Ñ Ïã†Î¢∞Î•º ÏåìÎäî Í∏∞ÌöåÍ∞Ä Îê† Ïàò ÏûàÎã§Îäî ÎúªÏù¥Í∏∞ÎèÑ ÌïòÎã§. Î∂ÅÏùÄ Í∑∏ÎèôÏïà Ïñ¥ÎñªÍ≤åÎì† Ïö∞Î¶¨Î•º Í±¥ÎÑàÎõ∞Ïñ¥ ÎØ∏Íµ≠Í≥º ÏßÅÍ±∞ÎûòÌï¥Î≥¥Î†§Îã§ Î≤àÎ≤àÏù¥ Ï¢åÏ†àÌñàÎã§. Î∂ÅÎèÑ Ïù¥Ï†úÎäî Ïô∏Î∂Ä ÏÑ∏Í≥ÑÏôÄ Ïà®ÌÜµÏùÑ ÌãîÏö∞Î†§Î©¥ ÎÇ®Î∂Å Í¥ÄÍ≥ÑÎ•º Ïñ¥Îäê Ï†ïÎèÑ Ï†ïÏÉÅ Í∂§ÎèÑÎ°ú ÎèåÎ†§ÎÜìÏßÄ ÏïäÏúºÎ©¥ Ïïà ÎêúÎã§Îäî Í≤ÉÏùÑ Íπ®Îã¨ÏïòÏùÑ Í≤ÉÏù¥Îã§.\", \"prompt\": \"Î∂ÅÏù¥ Ïô∏Î∂Ä ÏÑ∏Í≥ÑÏôÄ Ïà®ÌÜµÏùÑ ÌãîÏö∞Î†§Î©¥, ÎÇ®Î∂Å Í¥ÄÍ≥ÑÎ•º Ïñ¥Îäê Ï†ïÎèÑ Ï†ïÏÉÅ Í∂§ÎèÑÎ°ú ÎèåÎ†§ÎÜìÏßÄ ÏïäÏúºÎ©¥ Ïïà ÎêúÎã§.\"}, \"output\": 6.282845973968506}\n",
      "{\"id\": \"nikluge-2022-nli-test-000004\", \"input\": {\"context\": \"Ïù¥ ÎïåÎ¨∏Ïóê Î¨¥ÏùòÏãùÏ†ÅÏù∏ ÎáåÎäî ÏÜçÎèÑÎäî Îπ†Î•¥ÏßÄÎßå Ï†ïÍµêÌï®Ïù¥ Îñ®Ïñ¥ÏßÑÎã§. Í∑∏ÎûòÏÑú ÏûêÏ£º Ïã§ÏàòÎ•º Ï†ÄÏßÄÎ•∏Îã§. Î¨∏Ïû•ÏùÑ ÏùΩÏùÑ Îïå Í∑∏ ÎúªÏùÄ Ïù¥Ìï¥ÌñàÏßÄÎßå Ï≤†ÏûêÍ∞Ä ÌãÄÎ¶∞ Í≤ÉÏùÑ Î∞úÍ≤¨ÌïòÏßÄ Î™ªÌñàÎã§Î©¥ Í∑∏Í≤ÉÏùÄ Î¨¥ÏùòÏãùÏ†ÅÏù∏ ÎáåÍ∞Ä ÏùºÏùÑ Îπ†Î•¥Í≤å Ï≤òÎ¶¨ÌñàÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§.\", \"prompt\": \"Î¨∏Ïû•Ïùò Ï≤†ÏûêÍ∞Ä ÌãÄÎ†∏Îã§.\"}, \"output\": 6.207643032073975}\n",
      "{\"id\": \"nikluge-2022-nli-test-000005\", \"input\": {\"context\": \"Ï§ëÍµ≠ÎåÄÏÇ¨Í¥ÄÏù¥ ‚ÄúÍ≤ÄÏÇ¨ Ï∏°ÏóêÏÑú Ï†úÏ∂úÌïú Í≥µÎ¨∏ Îì± 3Í±¥Ïùò Î¨∏ÏÑúÎäî Î™®Îëê ÏúÑÏ°∞Îêú Í≤É‚ÄùÏù¥ÎùºÍ≥† Î∞ùÌòîÍ≥†, ÎåÄÌïúÎØºÍµ≠ Ïô∏ÍµêÎ∂Ä ÏàòÏû•Ïù¥ ‚ÄòÍ≥µÎ¨∏ÏÑú 2Í±¥ÏùÄ Ï†ïÏãù Ïô∏ÍµêÍ≤ΩÎ°úÎ•º ÌÜµÌïòÏßÄ ÏïäÏïòÎã§‚ÄôÍ≥† Î∞ùÌòîÏúºÎ©¥ ÏÇ¨Ïã§ÏÉÅ ‚ÄòÏÉÅÌô©ÏùÄ Ï¢ÖÎ£åÎêú Í≤É‚ÄôÏúºÎ°ú Î¥êÎèÑ ÎêúÎã§.\", \"prompt\": \"Í≥µÎ¨∏ÏÑú 2Í±¥ÏùÄ Ï†ïÏãù Ïô∏Íµê Í≤ΩÎ°úÎ•º ÌÜµÌïòÏßÄ ÏïäÏïòÎã§.\"}, \"output\": 6.1461663246154785}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# JSONL ÌååÏùºÎ°ú Ï†ÄÏû•\n",
    "output_path = './data/certainty-predictions-fin.jsonl'\n",
    "with open(output_path, 'w', encoding='utf-8') as file:\n",
    "    for item in output:\n",
    "        file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Ï≤´ 5Ï§Ñ Ï∂úÎ†•\n",
    "!head -n 5 ./data/certainty-predictions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   3472 MiB |  12970 MiB |  67702 GiB |  67699 GiB |\n",
      "|       from large pool |   3454 MiB |  12952 MiB |  67490 GiB |  67486 GiB |\n",
      "|       from small pool |     17 MiB |     23 MiB |    212 GiB |    212 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   3472 MiB |  12970 MiB |  67702 GiB |  67699 GiB |\n",
      "|       from large pool |   3454 MiB |  12952 MiB |  67490 GiB |  67486 GiB |\n",
      "|       from small pool |     17 MiB |     23 MiB |    212 GiB |    212 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   3471 MiB |  12969 MiB |  67702 GiB |  67698 GiB |\n",
      "|       from large pool |   3453 MiB |  12951 MiB |  67490 GiB |  67486 GiB |\n",
      "|       from small pool |     17 MiB |     23 MiB |    212 GiB |    212 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   3498 MiB |  13314 MiB | 477294 MiB | 473796 MiB |\n",
      "|       from large pool |   3480 MiB |  13294 MiB | 474904 MiB | 471424 MiB |\n",
      "|       from small pool |     18 MiB |     24 MiB |   2390 MiB |   2372 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  26176 KiB | 384437 KiB |  23641 GiB |  23641 GiB |\n",
      "|       from large pool |  25768 KiB | 382712 KiB |  23420 GiB |  23420 GiB |\n",
      "|       from small pool |    408 KiB |   7038 KiB |    221 GiB |    221 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1298    |    1652    |    2791 K  |    2790 K  |\n",
      "|       from large pool |     301    |     552    |    1937 K  |    1937 K  |\n",
      "|       from small pool |     997    |    1100    |     853 K  |     852 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1298    |    1652    |    2791 K  |    2790 K  |\n",
      "|       from large pool |     301    |     552    |    1937 K  |    1937 K  |\n",
      "|       from small pool |     997    |    1100    |     853 K  |     852 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     140    |     294    |    6380    |    6240    |\n",
      "|       from large pool |     131    |     284    |    5185    |    5054    |\n",
      "|       from small pool |       9    |      12    |    1195    |    1186    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      13    |      36    |    1525 K  |    1525 K  |\n",
      "|       from large pool |       5    |      13    |    1176 K  |    1176 K  |\n",
      "|       from small pool |       8    |      30    |     348 K  |     348 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string = torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "# stringÏùÑ 20Ï§ÑÏî© ÎÅäÏñ¥ÏÑú Ï∂úÎ†•\n",
    "string = string.split('\\n')\n",
    "for i in range(0, len(string), 20):\n",
    "    print('\\n'.join(string[i:i+20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "18th",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
